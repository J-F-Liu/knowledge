解析解(analytical solution)：是给出解的具体函数形式，从解的表达式中就可以算出任何对应值。

封闭解(Closed-form solution)：也被翻译为闭合解/闭式解，解析解是一个封闭形式(Closed-form) 的函数，因此对任一自变量，我们皆可将其带入解析函数求得正确的因变量。因此，解析解也被称为封闭解。

数值解(numerical solution)：是采用某种计算方法,如有限元的方法, 数值逼近,插值的方法, 得到的解。

当输入数据有测量误差时，即使有解析解，算出来的结果也是不精确的；仍需要通过数值计算的方法进行优化和逼近。

## 非线性方程组

There are no good, general methods for solving systems of more than one nonlinear equation. You will almost always have to use additional information, specific to your particular problem, to answer such basic questions as, “Do I expect a unique solution?” and “Approximately where?”

If you have a sufficiently good initial guess, there is method to converge to a root iteratively.

## 优化算法

LM算法是最小二乘问题的黄金标准（scipy中 least_squares 默认使用LM）。
L-BFGS也可用，但数值微分下LM通常更优。

数值微分法（如中心差分） 会引入额外误差，当数值微分误差 > 1e-4 时，LM 可保持收敛，L-BFGS 可能发散。
L-BFGS对初始点要求较高（需合理范围），LM算法有步长自适应机制，对初始点容忍度更高，更稳定。

L-BFGS(torch.optim.LBFGS)在PyTorch中通常用于小模型（≤10个参数），神经网络中不常见。

神经网络中的Tensor Autodiff（自动微分）不是数值微分，而是符号微分（Symbolic Differentiation）的高效实现（通过计算图推导解析梯度）。

自动微分的浮点误差（1e−15 级），但远低于数值微分（1e−3 级）。

LM 在非线性场景中更稳定，当基函数非线性较强（如 sin(x) 或 exp(x)）时，L-BFGS 有 ~30% 的概率在 10 次迭代内发散（尤其初始点不理想），而 LM 通过 λ 动态调整，几乎总是稳定收敛。

在 PyTorch 中，LM 算法不是内置优化器，但通过 scipy.optimize.least_squares（结合 PyTorch 的 Autodiff）可轻松实现。

Forward Auto-Differentiation (前向)每个节点计算输入对输出的导数，计算函数值的同时计算导数。
适用于输入维度小、输出维度大（n ≪ m）。内存消耗低，不需要存储中间图。

Reverse Auto-Differentiation (反向)从输出回溯到输入，先计算函数值 (Forward Pass)，再回传导数 (Backward Pass)。适用于输入维度大、输出维度小（n ≫ m）。内存消耗高，需要存储前向传播的中间值以备后用。

在机器学习中，通常是一个 Loss（1 个输出）对几百万个权重（n 个输入）。我们需要的是 1×n 的梯度行向量。因此，Reverse Mode 是深度学习的唯一选择，尽管它极其消耗显存（因为要存中间状态）。

在小规模参数的最小二乘问题中，AdamW 很难达到 LM（Levenberg-Marquardt）算法的精度和收敛速度。但在大规模参数（深度神经网络）场景下，LM 算法根本无法运行，AdamW 是最优解。

如果你的问题规模允许运行 LM，通常 LM 会得到比 AdamW 更精确（Loss 更低）的结果，且迭代次数少几个数量级。
通常 AdamW 的 Loss 会停留在比 LM 高几个数量级的地方（例如 LM 到了 1e-10，AdamW 还在 1e-5 徘徊）。

LM容易陷入尖锐的局部最小值（Sharp Minima）。在数据有噪或模型过参数化时，可能会过拟合。
AdamW倾向于寻找平坦的最小值（Flat Minima）。这对泛化能力（Generalization）更好，但单纯从 “拟合训练数据” 的角度看，Loss 可能不如 LM 低。

|参数量 | 推荐算法|
|-------|-------|
|< 1,000 | Autodiff + LM|
|1k - 10k| L-BFGS(拟牛顿法) 比 AdamW 精度高，但比 LM 省内存|
|> 100,000| LM 无法运行（内存溢出）, AdamW 是唯一可行解|

如果你是在做深度学习（分类、检测、NLP）：别想 LM 了，AdamW 是你的朋友。LM 算不动，且也没必要算那么准（容易过拟合）。

如果你是在做科学计算、参数反演、强约束拟合（参数少，要求精度高）：AdamW 通常达不到 LM 的优化结果。你应该优先选择 LM，或者至少使用 L - BFGS。

## 基函数
输入输出都是二维的，并假设是映射是平滑的，用哪些非线性的基函数？

1. 径向基函数 (RBF) —— 首选方案

它基于 “欧几里得距离”，天然适应 2D 平面上的散点分布，具有各向同性。

$$\phi_i(\mathbf{x}) = \exp(-\gamma \|\mathbf{x} - \mathbf{c}_i\|^2) \quad \text{(高斯核)}$$
或者
$$\phi_i(\mathbf{x}) = \sqrt{1 + (\epsilon \|\mathbf{x} - \mathbf{c}_i\|)^2} \quad \text{(多二次曲面 Multiquadric)}$$

$\mathbf{x}$ 是二维输入向量 $(x_1, x_2)$。$\mathbf{c}_i$ 是分布在平面上的中心点。

高斯核 (Gaussian) 是局部 (Local) 的，如果你的数据点比较稀疏，在两个数据点中间的区域，高斯核的叠加结果可能会塌陷接近于 0。这会导致拟合出来的曲面像是一堆 “小山包”，中间有凹陷。

高斯核收敛极快，适合分类问题或变化极其剧烈的局部特征，但在平滑曲面拟合中容易出现 “波纹”。

Multiquadric (MQ) 是全局 (Global) 的，它的形状是一个双曲面 (Hyperboloid)（类似于一个圆角的倒圆锥）。虽然单个 MQ 函数在远处趋向无穷大，但我们在拟合时是使用它们的线性组合，通过权重的正负抵消，它们可以在我们关注的区域内极其平滑地逼近目标曲面，而不会在数据稀疏的地方 “塌陷”。有点反直觉，反而常被认为是最好用的。

它是 Hardy 在 1971 年提出的，被称为 “最好的插值方法之一”。数学家 Micchelli (1986) 证明了，对于 Multiquadric 函数，只要数据点互不重合，插值矩阵 $A$ 保证是可逆的。这使得它在数值计算上非常稳健，不容易出现解不出来的情况。

Multiquadric (MQ) 作为 $r$ 的函数泰勒展开只有偶次项，但这完全不会限制它拟合奇函数（如 $x, x^3$）的能力。因为不是在原点拟合，而是使用多个位移（Shifted）后的基函数线性组合。

如果你非常介意发散特性，或者你的物理场景确实要求远处归零（例如电势场或概率密度），你应该使用的是 Inverse Multiquadric (IMQ)：$$\phi(r) = \frac{1}{\sqrt{1 + (\epsilon r)^2}}$$
它在尾部比高斯核衰减得更慢（重尾），因此 “感知范围” 比高斯核更广，通常比高斯核拟合效果更好。

2. 傅里叶特征映射 (Fourier Features) / SIREN (Sinusoidal Representation Networks) —— 现代最强方案

直接将神经网络的激活函数换成正弦函数：
$$\phi(x) = \sin(\omega x + b)$$

然后再输入到一个使用 Tanh 或 Swish 激活函数的平滑多层感知机 (MLP) 中。

普通神经网络倾向于学习低频（模糊）函数。使用正弦基函数可以让模型迅速捕捉高频细节，同时保持整体的平滑连续性。

SIREN的逻辑是：让网络的第一层自己变成频率生成器，并且层层叠加频率。它在整个网络深处持续使用正弦激活。
Fourier Features (FF)的逻辑是：先把坐标映射到高频空间，然后用普通网络去学。
如果你把 SIREN 的第一层权重 $\mathbf{W}_1$ 固定住不训练，并且后面接 ReLU 网络，那它本质上就退化成了 Fourier Features。

Fourier Features表现非常稳健。由于 $\mathbf{B}$ 矩阵固定，你可以通过控制 $\mathbf{B}$ 的分布（带宽）来明确控制输出的平滑度。由于后面接的是 ReLU，训练通常收敛更快，不容易跑飞。

场景 A：图像 / 3D 重建 (NeRF, 图像拟合) —— 可以替代
SIREN也能生成极高精度的图像。但它对初始化非常敏感（需要特殊的初始化方案），调节起来比 FF 稍微麻烦一点。

场景 B：求解偏微分方程 (PINNs, 物理模拟) —— SIREN 完胜
$\sin(x)$ 的导数是 $\cos(x)$，二阶导数是 $-\sin(x)$。无论求多少阶导，它都是平滑且非零的。这对物理方程极其重要。
FF + Tanh: 虽然 Tanh 平滑，但深层 Tanh 网络容易出现梯度消失，且拟合高频细节的能力不如全正弦的 SIREN。

场景 C：泛化与插值 (Generalization) —— Fourier Features 略胜
FF通过调整 $\mathbf{B}$ 的尺度（Scale），你可以像调节收音机旋钮一样，决定网络能学到的最高频率是多少。
SIREN倾向于利用全频段能力去 “死记硬背” 数据，有时候由于频率适应性太强，在数据间隙容易产生高频伪影（Artifacts）。

3. 二维正交多项式 (2D Tensor Product Polynomials) —— 经典有界方案

如果你的输入数据严格限制在一个方形区域内（例如 $[-1, 1] \times [-1, 1]$），使用二维切比雪夫多项式是非常数学正统的做法。

$$\phi_{i,j}(x, y) = T_i(x) \cdot T_j(y)$$
其中 $T_i$ 是 $i$ 阶切比雪夫多项式。

不需要像 RBF 那样去调整中心点位置 $\mathbf{c}_i$ 或宽度 $\epsilon$，只需要确定多项式的阶数。
但如果数据分布非常不均匀，或者区域是不规则形状（非矩形），拟合效果会大打折扣。

正交多项式（Orthogonal Polynomials）是科学计算和逼近理论中的 “特种部队”。与普通的幂函数基底 $\{1, x, x^2, \dots\}$ 相比，它们之间互不干扰（正交性），能避免数值计算中的 “病态” 问题，且收敛速度极快。
随着 $n$ 增大，$x^n$ 和 $x^{n-1}$ 的函数图像在区间 $[0, 1]$ 长得非常像。$x^5$ 和 $x^6$ 在图上几乎是重叠的。
设计矩阵列向量之间线性相关性极强，条件数呈指数级爆炸，数值噪声会被放大亿万倍，甚至无法求逆。
而$T_n(x)$ 在区间内不断振荡，正负交替。$T_5$ 和 $T_6$ 的形状截然不同。这意味着矩阵的列向量是 “相互独立” 的。

1. 切比雪夫多项式 (Chebyshev Polynomials) —— 逼近之王
误差在整个区间内分布最均匀（等波纹特性）。
它能保证在整个矩形框内误差最均匀，不会出现 “中心拟合很好，四个角乱飞” 的情况。
可以通过 FFT（快速傅里叶变换）进行 $O(N \log N)$ 的快速变换。

2. 勒让德多项式 (Legendre Polynomials) —— 几何与物理的标准
在 $L_2$ 范数（最小二乘）意义下是正交的。它是对区间内所有点 “一视同仁” 的最佳拟合。
它是球坐标系中拉普拉斯方程解的一部分（球谐函数），对应物理中的多极展开。

3. 厄米特多项式 (Hermite Polynomials) —— 无界与概率
它是量子谐振子的本征函数，隐含了正态分布（高斯函数）作为权重。
它非常擅长拟合那些 “在原点附近震荡，但在远处迅速衰减为 0” 的函数。

4. 拉盖尔多项式 (Laguerre Polynomials) —— 半边天与时间
适用于处理定义在正半轴上，且随 $x$ 增大而指数衰减的函数。

5. 泽尼克多项式 (Zernike Polynomials) —— 2D 圆盘专用
定义域为 单位圆盘 $x^2 + y^2 \le 1$，直接使用极坐标 $(\rho, \theta)$ 定义。

如果你要拟合的数据正好在一个圆形范围内（如晶圆、瞳孔、透镜），这是绝对的神器。

描述透镜的特性，但采样数据来自矩形的感光芯片：

如果你是为了分析透镜质量（如：想知道有多少像散、彗差、球差），必须用 泽尼克 (Zernike)，但需要特殊处理。

如果你是为了校正 / 拟合误差（如：做图像的去畸变、标定、或者光刻机的波面补偿），完全不在乎物理含义，请坚决使用 切比雪夫 (Chebyshev)。

在矩形芯片上直接使用标准 Zernike 会导致正交性丧失 (Loss of Orthogonality)，泽尼克多项式只有在完整的圆盘上积分才是正交的。

### 切比雪夫多项式
第一类切比雪夫多项式，是在函数拟合、插值和逼近中最常用的一类，因为它们的极值点分布均匀。

$T_0(x)$和$T_1(x)$是初始项，$T_n(x)$递推公式：
$$T_{n+1}(x) = 2x T_n(x) - T_{n-1}(x)$$ 
前几项为：
$$
\begin{aligned}
T_0(x) &= 1\\
T_1(x) &= x\\
T_2(x) &= 2x^2 - 1\\
T_3(x) &= 4x^3 - 3x\\
T_4(x) &= 8x^4 - 8x^2 + 1\\
T_5(x) &= 16x^5 - 20x^3 + 5x\\
\end{aligned}
$$

[Chebyshev Series](https://www.desmos.com/calculator/u32i1obzrh)

$T_n(x)$的奇偶性与$n$相关：
- 当$n$为偶数时，$T_n(x)$为偶函数。
- 当$n$为奇数时，$T_n(x)$为奇函数。

对于 2D 拟合，基函数是 $x$ 方向和 $y$ 方向切比雪夫多项式的乘积：$$\Phi_{ij}(x, y) = T_i(x) \cdot T_j(y)$$其中 $i$ 和 $j$ 分别是 $x$ 和 $y$ 方向的阶数。

设最高阶数为 $N$（例如 $N=3$ 或 $4$），有两种组合基函数的方式：
1. 张量积型 (Tensor Product): $0 \le i, j \le N$。
包含所有交叉项，如 $T_N(x)T_N(y)$。
2. 总阶数型 (Total Degree): $0 \le i + j \le N$。
去除高频交叉项，参数更少，更抗过拟合（推荐）。

对于每一个样本点 $k$，模型可以写为：
$$
u_k = \sum_{i,j} c_{ij}^{(u)} \cdot T_i(x_k) T_j(y_k) \\
v_k = \sum_{i,j} c_{ij}^{(v)} \cdot T_i(x_k) T_j(y_k)
$$
注意：$x, y$ 的基函数是共享的，但系数 $c^{(u)}$ 和 $c^{(v)}$ 是独立的。
用矩阵方程求解系数，拟合$(u,v)=f(x,y)$。

取$N=5$，则基函数为：
$$
\begin{align}
\Phi_{00}(x, y) &= T_0(x) \cdot T_0(y) = 1\\
\Phi_{10}(x, y) &= T_1(x) \cdot T_0(y) = x\\
\Phi_{01}(x, y) &= T_0(x) \cdot T_1(y) = y\\
\Phi_{20}(x, y) &= T_2(x) \cdot T_0(y) = 2x^2 - 1\\
\Phi_{11}(x, y) &= T_1(x) \cdot T_1(y) = xy\\
\Phi_{02}(x, y) &= T_0(x) \cdot T_2(y) = 2y^2 - 1\\
\Phi_{30}(x, y) &= T_3(x) \cdot T_0(y) = 4x^3 - 3x\\
\Phi_{21}(x, y) &= T_2(x) \cdot T_1(y) = 2x^2 y - y\\
\Phi_{12}(x, y) &= T_1(x) \cdot T_2(y) = 2x y^2 - x\\
\Phi_{03}(x, y) &= T_0(x) \cdot T_3(y) = 4y^3 - 3y\\
\Phi_{40}(x, y) &= T_4(x) \cdot T_0(y) = 8x^4 - 8x^2 + 1\\
\Phi_{31}(x, y) &= T_3(x) \cdot T_1(y) = 4x^3 y - 3x y\\
\Phi_{22}(x, y) &= T_2(x) \cdot T_2(y) = 4x^2 y^2 - 2x^2 - 2y^2 + 1\\
\Phi_{13}(x, y) &= T_1(x) \cdot T_3(y) = 4x y^3 - 3xy\\
\Phi_{04}(x, y) &= T_0(x) \cdot T_4(y) = 8y^4 - 8y^2 + 1\\
\Phi_{50}(x, y) &= T_5(x) \cdot T_0(y) = 16x^5 - 20x^3 + 5x\\
\Phi_{41}(x, y) &= T_4(x) \cdot T_1(y) = 8x^4y - 8x^2y + y\\
\Phi_{32}(x, y) &= T_3(x) \cdot T_2(y) = 8x^3y^2 - 6xy^2 - 4x^3 + 3x\\
\Phi_{23}(x, y) &= T_2(x) \cdot T_3(y) = 8x^2y^3 - 6x^2y - 4y^3 + 3y\\
\Phi_{14}(x, y) &= T_1(x) \cdot T_4(y) = 8xy^4 - 8xy^2 + x\\
\Phi_{05}(x, y) &= T_0(x) \cdot T_5(y) = 16y^5 - 20y^3 + 5y\\
\end{align}
$$

把 $T_n(x)$ 展开成 $x^n$ 时，系数会变得非常巨大且正负交替。用很大的数相减，最后得到一个很小的数，这被称为灾难性抵消。
在 $N=4$ 或 $5$ 时，把它转成幂基，系数膨胀尚不严重，双精度（double）扛得住。
推荐方案是不要转换，直接求值。

切比雪夫多项式有 4 类，递推规则都一样，只是初始项不一样, 分别是：
1. 第一类切比雪夫多项式：$T_0(x) = 1, T_1(x) = x$
2. 第二类切比雪夫多项式：$T_0(x) = 1, T_1(x) = 2x$
3. 第三类切比雪夫多项式：$T_0(x) = x, T_1(x) = 2x - 1$
4. 第四类切比雪夫多项式：$T_0(x) = x, T_1(x) = 2x + 1$

核心区别在于它们的正交性权函数（Weight Function）不同。
|类别|符号|权函数 $w(x)$| 特点 (端点)| 什么时候用？|
|----|----|------------|-----------|-----------|
|第一类| $T_n$​|$\frac{1}{\sqrt{1-x^2}}$| 两端都发散|通用拟合、插值、逼近（95% 的情况用这个）
|第二类| $U_n$​|$\sqrt{1-x^2}$| 两端都为 0| 数值积分、函数在端点为 0 时
|第三类| $V_n$​|$\sqrt{\frac{1+x}{1-x}}$​|右端发散，左端 0| 非对称边界（右边难处理，左边简单）
|第四类| $W_n$​|$\sqrt{\frac{1-x}{1+x}}$​|左端发散，右端 0| 非对称边界（左边难处理，右边简单）

切比雪夫多项式的另一种定义：
$$
T_n(x) = \begin{cases}
\cos(n \arccos(x)) &\text{当 |x| ⩽ 1 时} \\
\cosh(n \text{arccosh}(x)) &\text{当 |x| > 1 时}
\end{cases}
$$
在低阶（如 N = 5）情况下，三角函数的计算比递推法慢 10-50 倍。

#### 数据点的取值范围 (x,-0.6,0.6), (y,-0.5,0.5)，使用第一类切比雪夫多项式，需要对数据点做额外的处理吗
虽然你的数据范围（$[-0.6, 0.6]$ 和 $[-0.5, 0.5]$）已经在 $[-1, 1]$ 之内，直接带入计算程序不会报错，但在数学和数值计算层面，不做归一化是一种 “浪费”，且会降低拟合质量。

如果你将范围拉伸到完整的 $[-1, 1]$，基函数之间的独立性最强，生成的矩阵条件数最小（数值最稳定）。如果不拉伸，基函数在局部范围内可能会表现得比较 “相似”（线性相关性增加），尤其是在高阶时。

要想直接输入原数据 $(x, y)$ 而不需要在代码里每次都手动除以 0.6 或 0.5，唯一的办法是将切比雪夫系数转换为普通多项式（幂基）系数（即 $1, x, y, x^2, xy, \dots$ 的形式），并在转换过程中把缩放因子 “吸收” 进系数里。

这样做会牺牲切比雪夫多项式的数值稳定性。但如果阶数 $N \le 5$，且为了部署方便，这是完全可行的。

#### 求矩阵的最小二乘解就可以得到多项式的系数，在哪个步骤需要用到 LM 算法呢？

在标准的切比雪夫多项式拟合中，不需要用到 Levenberg-Marquardt (LM) 算法。
系数 $\mathbf{C}$ 与模型之间是线性关系：
$$F(\hat{x}, \hat{y}) = \sum C_{ij} T_i(\hat{x}) T_j(\hat{y})$$
直接用矩阵运算 SVD 法解方程 $(\mathbf{A}^T\mathbf{A})\mathbf{c} = \mathbf{A}^T\mathbf{F}$

如果参数出现在多项式 “内部”，则需要用 LM 算法。
$$y = \sum c_i T_i( \mathbf{k} \cdot x + \mathbf{b} )$$
这里的 $k$ 和 $b$ 被包裹在 $T_i(\dots)$ 函数里面，变成了非线性优化问题。

标准的最小二乘是最小化 $y$ 方向的误差 $(y_{pred} - y_{true})^2$。如果你是在做计算机图形学，想让点到曲面的垂直欧几里得距离（Orthogonal Distance）最小。
对于复杂曲面，计算点到曲面的垂直距离本身就是非线性的，必须用 LM 算法不断调整曲面参数，使几何距离误差和最小。

二维双线性插值
https://www.desmos.com/3d/owiispjgys