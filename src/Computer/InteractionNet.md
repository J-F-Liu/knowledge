What is the true nature of computation? A hundred years ago, humanity answered that very question, twice.

In 1936, Alan invented the Turing Machine, which, highly inspired by the mechanical trend of the 20th century, distilled the common components of early computers into a single universal machine that, despite its simplicity, was capable of performing every computation conceivable. From simple numerical calculations to entire operating systems, this small machine could compute anything. Thanks to its elegance and simplicity, the Turing Machine became the most popular model of computation, and served as the main inspiration behind every modern processor and programming language. C, Fortran, Java and Python, are languages based on a procedural mindset, which is highly inspired by Turing's invention.

Aslo in 1936, Alonzo Church invented the Lambda Calculus, which distilled the common components of different branches of math - into a single universal language that was capable of modeling every mathematical theory. What was surprising, though, is that this language, unexpectedly, could also perform computations. The same algorithms that could be computed by Turing Machines procedurally, could also be computed by the Lambda Calculus, through symbolic manipulations. The idea of using the Lambda Calculus for computations inspired the creation of an entire new branch of programming, which we call the functional paradigm. Haskell, Clojure, Elixir and Agda, are languages based on the functional mindset, which is highly inspired by Church's invention.

It was proven that, when it comes to computability, Turing Machines and the Lambda Calculus are equivalent. Every problem that one can solve, can also be solved by the other. That insight is known as the Church-Turing thesis, which essentially states that computers are capable of emulating each-other.

Yet, while the Church-Turing hypothesis makes a statement about computability, it says nothing about computation. In other words, a model can be inherently less efficient than other. Historically, procedural languages such as C and Fortran have consistently outperformed the fastest functional languages, such as Haskell and Ocaml. Yet, languages like Haskell and Agda provide abstractions that make entire classes of bugs unrepresentable.

This raises the question: is there a model of computation, which, like the Turing Machine, has a reasonable physical implementation, and yet, like the Lambda Calculus, has a robust logical interpretation?

In 1997, Yves Lafont proposed a new alternative, the Interaction Combinators, on which substitution is broken down into 2 fundamental laws: commutation, which creates and copies information, and annihilation, which observes and destroys information. The charm, and elegance, of the Interaction Combinators is that its reduction laws are truly atomic: each operation can be completed in a constant amount of steps, and has a clear physical mapping. Not only that, they're inherently parallel.

Interestingly, every aspect that is considered good in other models of computation, is present on Interaction Combinators, while negative aspects are completely absent. Moreover, both the Lambda Calculus and the Turing Machine can be efficiently emulated by the Interaction Combinators, while the opposite isn't true. This suggests that, while the 3 systems are equivalent in terms of computability, the Interaction Combinators are more capable in terms of computation.

A model of computation has a significant impact on the way we design our languages, and several of their drawbacks can be traced down to limitations of the underlying model. Consequently, that computers, processors and programming languages inspired by Interaction Combinators would offer tangible benefits compared to the ones we built based on Turing Machines and the Lambda Calculus.
